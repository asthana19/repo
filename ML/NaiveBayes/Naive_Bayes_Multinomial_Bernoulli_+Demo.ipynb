{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial and Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For understanding Multinomial and Bernoulli Naive Bayes, we will start with a small example and understand the end to end process. In another notebook, we will build a full-fledged email spam classifier.\n",
    "\n",
    "To start with, let's take a few sentences and classify them in two different classes - *education* or *cinema*. Each sentence will represent one document. In real-world cases, a document be any piece of text such as an email, a news article, a book review, a tweet etc.\n",
    "The analysis and the algorithm involved doesn’t depend on the type of document we use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook is divided into the following sections:\n",
    "1. Importing and preprocessing data\n",
    "2. Building the model: Multinomial Naive Bayes\n",
    "3. Building the model: Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing and Preprocessing Data\n",
    "\n",
    "Let us first look at the sentences and their classes. We have kept the training sentences in file example_train.csv. Test sentences have been put in the file example_test.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pos</td>\n",
       "      <td>a common complaint amongst film critics is   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pos</td>\n",
       "      <td>whew   this film oozes energy   the kind of b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pos</td>\n",
       "      <td>steven spielberg s   amistad     which is bas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pos</td>\n",
       "      <td>he has spent his entire life in an awful litt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pos</td>\n",
       "      <td>being that it is a foreign language film with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>Neg</td>\n",
       "      <td>if anything     stigmata   should be taken as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>Neg</td>\n",
       "      <td>john boorman s   zardoz   is a goofy cinemati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>Neg</td>\n",
       "      <td>the kids in the hall are an acquired taste   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>Neg</td>\n",
       "      <td>there was a time when john carpenter was a gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599</th>\n",
       "      <td>Neg</td>\n",
       "      <td>two party guys bob their heads to haddaway s ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     class                                               text\n",
       "0      Pos   a common complaint amongst film critics is   ...\n",
       "1      Pos   whew   this film oozes energy   the kind of b...\n",
       "2      Pos   steven spielberg s   amistad     which is bas...\n",
       "3      Pos   he has spent his entire life in an awful litt...\n",
       "4      Pos   being that it is a foreign language film with...\n",
       "...    ...                                                ...\n",
       "1595   Neg   if anything     stigmata   should be taken as...\n",
       "1596   Neg   john boorman s   zardoz   is a goofy cinemati...\n",
       "1597   Neg   the kids in the hall are an acquired taste   ...\n",
       "1598   Neg   there was a time when john carpenter was a gr...\n",
       "1599   Neg   two party guys bob their heads to haddaway s ...\n",
       "\n",
       "[1600 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "# training data\n",
    "train_docs = pd.read_csv('movie_review_train.csv') \n",
    "train_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as you can see there are 5 documents (sentences) , 3 are of \"education\" class and 2 are of \"cinema\" class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Neg    800\n",
       "Pos    800\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pos_neg = train_docs['class'].value_counts()\n",
    "pos_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>a common complaint amongst film critics is   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>whew   this film oozes energy   the kind of b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>steven spielberg s   amistad     which is bas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>he has spent his entire life in an awful litt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>being that it is a foreign language film with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>0</td>\n",
       "      <td>if anything     stigmata   should be taken as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>0</td>\n",
       "      <td>john boorman s   zardoz   is a goofy cinemati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>0</td>\n",
       "      <td>the kids in the hall are an acquired taste   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>0</td>\n",
       "      <td>there was a time when john carpenter was a gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599</th>\n",
       "      <td>0</td>\n",
       "      <td>two party guys bob their heads to haddaway s ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      class                                               text\n",
       "0         1   a common complaint amongst film critics is   ...\n",
       "1         1   whew   this film oozes energy   the kind of b...\n",
       "2         1   steven spielberg s   amistad     which is bas...\n",
       "3         1   he has spent his entire life in an awful litt...\n",
       "4         1   being that it is a foreign language film with...\n",
       "...     ...                                                ...\n",
       "1595      0   if anything     stigmata   should be taken as...\n",
       "1596      0   john boorman s   zardoz   is a goofy cinemati...\n",
       "1597      0   the kids in the hall are an acquired taste   ...\n",
       "1598      0   there was a time when john carpenter was a gr...\n",
       "1599      0   two party guys bob their heads to haddaway s ...\n",
       "\n",
       "[1600 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert label to a numerical variable\n",
    "train_docs['class'] = train_docs['class'].map({'Neg':0, 'Pos':1})\n",
    "train_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>a common complaint amongst film critics is   w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>whew   this film oozes energy   the kind of br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>steven spielberg s   amistad     which is base...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>he has spent his entire life in an awful littl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>being that it is a foreign language film with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>0</td>\n",
       "      <td>if anything     stigmata   should be taken as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>0</td>\n",
       "      <td>john boorman s   zardoz   is a goofy cinematic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>0</td>\n",
       "      <td>the kids in the hall are an acquired taste    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>0</td>\n",
       "      <td>there was a time when john carpenter was a gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599</th>\n",
       "      <td>0</td>\n",
       "      <td>two party guys bob their heads to haddaway s d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      class                                               text\n",
       "0         1  a common complaint amongst film critics is   w...\n",
       "1         1  whew   this film oozes energy   the kind of br...\n",
       "2         1  steven spielberg s   amistad     which is base...\n",
       "3         1  he has spent his entire life in an awful littl...\n",
       "4         1  being that it is a foreign language film with ...\n",
       "...     ...                                                ...\n",
       "1595      0  if anything     stigmata   should be taken as ...\n",
       "1596      0  john boorman s   zardoz   is a goofy cinematic...\n",
       "1597      0  the kids in the hall are an acquired taste    ...\n",
       "1598      0  there was a time when john carpenter was a gre...\n",
       "1599      0  two party guys bob their heads to haddaway s d...\n",
       "\n",
       "[1600 rows x 2 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert label to a numerical variable\n",
    "train_docs['text'] = train_docs['text'].str.lstrip()\n",
    "train_docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now split the dataframe into X and y labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train\n",
      "[1 1 1 ... 0 0 0]\n",
      "y_train\n",
      "[b'a comm' b'whew  ' b'steven' ... b'the ki' b'there ' b'two pa']\n"
     ]
    }
   ],
   "source": [
    "# convert the df to a numpy array \n",
    "train_array = train_docs.values\n",
    "\n",
    "# split X and y\n",
    "X_train = train_array[:,0]\n",
    "y_train = train_array[:,1]\n",
    "\n",
    "# sklearn needs y as integers\n",
    "#y_train = y_train.astype('int')\n",
    "y_train = np.asarray(y_train, dtype=\"|S6\")\n",
    "print(\"X_train\")\n",
    "print(X_train)\n",
    "print(\"y_train\")\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Bag of Words Representation\n",
    "\n",
    "We now have to convert the data into a format which can be used for training the model. We'll use the **bag of words representation** for each sentence (document).\n",
    "\n",
    "Imagine breaking X in individual words and putting them all in a bag. Then we pick all the unique words from the bag one by one and make a dictionary of unique words. \n",
    "\n",
    "This is called **vectorization of words**. We have the class ```CountVectorizer()``` in scikit learn to vectorize the words. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an object of CountVectorizer() class \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "# help(CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(stop_words='english',min_df=.03, max_df=.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here ```vec``` is an object of class ```CountVectorizer()```. This has a method called  ```fit()``` which converts a corpus of documents to a matrix of 'tokens'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'comm': 119,\n",
       " 'whew': 591,\n",
       " 'steven': 502,\n",
       " 've': 566,\n",
       " 'hate': 243,\n",
       " 'susan': 514,\n",
       " 'ingred': 262,\n",
       " 'capsul': 93,\n",
       " 'bruce': 86,\n",
       " 'mus': 362,\n",
       " 'ca': 91,\n",
       " 'expand': 186,\n",
       " 'waitin': 574,\n",
       " 'becaus': 67,\n",
       " '197': 6,\n",
       " 'sha': 473,\n",
       " 'today': 539,\n",
       " 'note': 372,\n",
       " 'bug': 89,\n",
       " 'near': 366,\n",
       " 'jerry': 279,\n",
       " 'ha': 240,\n",
       " 'da': 136,\n",
       " 'seen': 466,\n",
       " 'althou': 37,\n",
       " 'fee': 195,\n",
       " 'ta': 520,\n",
       " 'perhap': 395,\n",
       " 'kadosh': 289,\n",
       " 'fi': 199,\n",
       " 'hum': 256,\n",
       " 'good': 230,\n",
       " 'curdle': 135,\n",
       " 'jarvis': 275,\n",
       " 'richar': 444,\n",
       " 'bree': 83,\n",
       " 'titani': 537,\n",
       " 'americ': 39,\n",
       " '19': 4,\n",
       " 'wa': 573,\n",
       " 'ma': 323,\n",
       " 'arg': 48,\n",
       " 'felix': 198,\n",
       " 'know': 296,\n",
       " 'living': 315,\n",
       " 'oh': 380,\n",
       " 'writte': 610,\n",
       " 'octobe': 379,\n",
       " 'ok': 381,\n",
       " 'traili': 546,\n",
       " 'll': 316,\n",
       " 'accord': 19,\n",
       " 'nottin': 375,\n",
       " 'slaver': 483,\n",
       " 'reflec': 433,\n",
       " 'mu': 359,\n",
       " 'gr': 234,\n",
       " 'lean': 306,\n",
       " '1992': 11,\n",
       " 'expect': 187,\n",
       " 'je': 277,\n",
       " 'aggres': 26,\n",
       " 'gothic': 233,\n",
       " 'ess': 180,\n",
       " 'clue': 117,\n",
       " 'dr': 166,\n",
       " 'people': 394,\n",
       " 'ce': 100,\n",
       " 'synops': 519,\n",
       " 'matthe': 334,\n",
       " 'ro': 446,\n",
       " 'confes': 120,\n",
       " 'john': 285,\n",
       " 'fu': 216,\n",
       " 'produc': 417,\n",
       " 'reme': 434,\n",
       " 'review': 442,\n",
       " 'fil': 201,\n",
       " 'ne': 365,\n",
       " 'contac': 122,\n",
       " 'like': 313,\n",
       " 'bo': 76,\n",
       " 'jonath': 286,\n",
       " 'scream': 464,\n",
       " 'leonar': 307,\n",
       " 'voices': 572,\n",
       " 'alien': 34,\n",
       " 'ultra': 555,\n",
       " 'elmore': 174,\n",
       " 'bob': 77,\n",
       " 'th': 528,\n",
       " 'martin': 331,\n",
       " 'carolc': 95,\n",
       " 'par': 390,\n",
       " 'contra': 123,\n",
       " 'elizab': 173,\n",
       " 'making': 325,\n",
       " 'set': 469,\n",
       " 'ex': 185,\n",
       " 'en': 175,\n",
       " 'someti': 487,\n",
       " 'didn': 154,\n",
       " '118': 0,\n",
       " 'mi': 343,\n",
       " 'okay': 382,\n",
       " 'taking': 521,\n",
       " 'garry': 217,\n",
       " 'reca': 431,\n",
       " 'oliver': 384,\n",
       " 'tim': 534,\n",
       " 'bowfin': 80,\n",
       " 'pulp': 419,\n",
       " 'gattac': 218,\n",
       " 'availa': 59,\n",
       " 'melvin': 338,\n",
       " 'plot': 404,\n",
       " 'defend': 145,\n",
       " 'jake': 272,\n",
       " '199': 9,\n",
       " 'shakes': 475,\n",
       " 'ye': 612,\n",
       " 'bri': 84,\n",
       " 'som': 486,\n",
       " 'fel': 197,\n",
       " 'george': 221,\n",
       " 'countr': 129,\n",
       " 'freque': 212,\n",
       " 'psycho': 418,\n",
       " 'jackie': 270,\n",
       " 'direct': 157,\n",
       " 'disney': 162,\n",
       " 'ri': 443,\n",
       " 'ladies': 301,\n",
       " 'billed': 71,\n",
       " 'apr': 46,\n",
       " 'based': 63,\n",
       " 'quenti': 421,\n",
       " 'david': 140,\n",
       " 'seven': 470,\n",
       " 'ast': 55,\n",
       " 'lake': 303,\n",
       " 'lot': 320,\n",
       " 'desper': 150,\n",
       " 'wond': 598,\n",
       " 'scarfa': 462,\n",
       " 'trekki': 548,\n",
       " 'polloc': 408,\n",
       " 'warnin': 580,\n",
       " 'sho': 476,\n",
       " 'deep': 144,\n",
       " 'vampir': 563,\n",
       " 'swashb': 516,\n",
       " 'rebel': 430,\n",
       " 'cather': 99,\n",
       " 'love': 321,\n",
       " 'cost': 127,\n",
       " 'jay': 276,\n",
       " 'sci': 463,\n",
       " 'let': 308,\n",
       " 'rated': 425,\n",
       " 'thi': 529,\n",
       " 'saving': 458,\n",
       " '50': 14,\n",
       " 'tr': 545,\n",
       " 'apocal': 43,\n",
       " 'earlie': 169,\n",
       " 'just': 288,\n",
       " 'barely': 62,\n",
       " 'want': 577,\n",
       " 'losing': 319,\n",
       " 'muc': 360,\n",
       " 'po': 406,\n",
       " 'gues': 238,\n",
       " 'maybe': 335,\n",
       " 'krippe': 298,\n",
       " 'si': 479,\n",
       " 'luckil': 322,\n",
       " 'cir': 112,\n",
       " 'nosfer': 370,\n",
       " 'sick': 480,\n",
       " 'studio': 505,\n",
       " 'james': 274,\n",
       " 'ene': 176,\n",
       " 'tbwp': 524,\n",
       " 'don': 164,\n",
       " 'gettin': 223,\n",
       " 'mat': 333,\n",
       " 'danger': 137,\n",
       " 'imagin': 260,\n",
       " 'steve': 501,\n",
       " 'fir': 205,\n",
       " 'harmle': 242,\n",
       " 'god': 228,\n",
       " 'se': 465,\n",
       " 'aliens': 35,\n",
       " 'metro': 342,\n",
       " 'eric': 178,\n",
       " 'hedwig': 246,\n",
       " 'underr': 557,\n",
       " 'fai': 191,\n",
       " 'sta': 494,\n",
       " 'eyes': 188,\n",
       " 'jamaic': 273,\n",
       " 'denzel': 147,\n",
       " 'whenev': 589,\n",
       " 'rememb': 435,\n",
       " 'errol': 179,\n",
       " 'summar': 510,\n",
       " 'hollyw': 252,\n",
       " 'say': 460,\n",
       " 'kevin': 291,\n",
       " 'rent': 438,\n",
       " 'everyo': 183,\n",
       " 'oct': 378,\n",
       " 'brian': 85,\n",
       " 'cri': 133,\n",
       " 'magnol': 324,\n",
       " 'grou': 236,\n",
       " 'mickey': 345,\n",
       " 'mira': 349,\n",
       " 'welc': 585,\n",
       " 'origin': 387,\n",
       " 'did': 153,\n",
       " 'yo': 616,\n",
       " 'feel': 196,\n",
       " 'ra': 423,\n",
       " 'tarzan': 523,\n",
       " 'castin': 98,\n",
       " 'histor': 250,\n",
       " 'natura': 364,\n",
       " 'jean': 278,\n",
       " 'pr': 412,\n",
       " 'mig': 347,\n",
       " 'kirk': 294,\n",
       " 'man': 326,\n",
       " 'su': 506,\n",
       " 'jacque': 271,\n",
       " 'findin': 204,\n",
       " 'supp': 512,\n",
       " 'tone': 542,\n",
       " 'op': 386,\n",
       " 'dec': 143,\n",
       " 'la': 299,\n",
       " 'renown': 437,\n",
       " '198': 7,\n",
       " 'look': 318,\n",
       " 'pleasa': 403,\n",
       " 'big': 70,\n",
       " 'time': 535,\n",
       " 'charli': 103,\n",
       " 'field': 200,\n",
       " 'alchem': 31,\n",
       " 'wo': 595,\n",
       " 'anothe': 42,\n",
       " 'freq': 211,\n",
       " 'ch': 102,\n",
       " 'mary': 332,\n",
       " 'touchs': 544,\n",
       " 'run': 451,\n",
       " 'pre': 413,\n",
       " 'star': 496,\n",
       " 'usuall': 562,\n",
       " 'priv': 415,\n",
       " 'sam': 455,\n",
       " 'copyri': 126,\n",
       " 'playwr': 402,\n",
       " 'titant': 538,\n",
       " 'sw': 515,\n",
       " 'modern': 353,\n",
       " 'vannes': 564,\n",
       " 'kn': 295,\n",
       " 'coup': 130,\n",
       " 'pa': 389,\n",
       " 'dimens': 156,\n",
       " 'edward': 172,\n",
       " 'movi': 355,\n",
       " 'life': 312,\n",
       " 'kolya': 297,\n",
       " 'anna': 41,\n",
       " 'devote': 152,\n",
       " 'film': 202,\n",
       " 'quiz': 422,\n",
       " 'jim': 282,\n",
       " 'men': 339,\n",
       " 'ralph': 424,\n",
       " 'true': 550,\n",
       " 'dark': 138,\n",
       " 'recent': 432,\n",
       " 'roboco': 449,\n",
       " 'probab': 416,\n",
       " 'cinema': 111,\n",
       " 'chr': 107,\n",
       " 'le': 305,\n",
       " 'tempe': 526,\n",
       " 'fa': 189,\n",
       " 'fritz': 215,\n",
       " 'vetera': 568,\n",
       " 'satiri': 457,\n",
       " 'cl': 114,\n",
       " 'drivin': 168,\n",
       " 'mars': 329,\n",
       " 'showgi': 478,\n",
       " 'unh': 559,\n",
       " 'stran': 504,\n",
       " 'albert': 30,\n",
       " 'mo': 352,\n",
       " 'razor': 426,\n",
       " 'pitch': 400,\n",
       " 'cho': 106,\n",
       " 'assume': 54,\n",
       " 'qu': 420,\n",
       " 'bea': 66,\n",
       " 'sm': 484,\n",
       " 'warren': 581,\n",
       " 'clint': 116,\n",
       " 'meet': 336,\n",
       " 'buffal': 88,\n",
       " 'sum': 509,\n",
       " 'mike': 348,\n",
       " 'ind': 261,\n",
       " 'throug': 531,\n",
       " 'wong': 600,\n",
       " 'city': 113,\n",
       " 'robert': 447,\n",
       " 'rog': 450,\n",
       " 'labell': 300,\n",
       " 'airpla': 28,\n",
       " 'apollo': 44,\n",
       " 'carla': 94,\n",
       " 'cine': 110,\n",
       " '191': 5,\n",
       " 'st': 493,\n",
       " 'niagar': 369,\n",
       " 'notice': 374,\n",
       " 'actu': 20,\n",
       " 'gere': 222,\n",
       " 'wyatt': 611,\n",
       " 'zoo': 617,\n",
       " 'billy': 72,\n",
       " 'tibet': 533,\n",
       " 'swea': 517,\n",
       " 'armage': 49,\n",
       " 'id': 258,\n",
       " 'meteor': 341,\n",
       " 'books': 79,\n",
       " 'eddie': 171,\n",
       " 'uncomp': 556,\n",
       " 'ent': 177,\n",
       " 'gordon': 232,\n",
       " 'anasta': 40,\n",
       " 'marie': 328,\n",
       " 'insane': 264,\n",
       " 'allen': 36,\n",
       " 'trees': 547,\n",
       " 'wish': 593,\n",
       " 'lisa': 314,\n",
       " 'thri': 530,\n",
       " 'wow': 607,\n",
       " 'glory': 227,\n",
       " 'truman': 551,\n",
       " 'jessic': 280,\n",
       " 'certai': 101,\n",
       " 'woul': 606,\n",
       " 'libby': 310,\n",
       " 'far': 193,\n",
       " 'try': 552,\n",
       " 'workin': 604,\n",
       " 'spice': 491,\n",
       " 'read': 427,\n",
       " 'shou': 477,\n",
       " 'bad': 60,\n",
       " 'darren': 139,\n",
       " 'int': 266,\n",
       " 'sens': 467,\n",
       " 'battle': 65,\n",
       " 'hey': 247,\n",
       " 'movies': 357,\n",
       " 'sh': 472,\n",
       " 'ar': 47,\n",
       " 'mercur': 340,\n",
       " 'hots': 254,\n",
       " 'sp': 489,\n",
       " 'ti': 532,\n",
       " 'ob': 377,\n",
       " 'porter': 409,\n",
       " 'wou': 605,\n",
       " 'onl': 385,\n",
       " 'tv': 553,\n",
       " '13': 1,\n",
       " 'alexan': 33,\n",
       " '8mm': 16,\n",
       " 'carry': 96,\n",
       " 'tomb': 540,\n",
       " 'walt': 576,\n",
       " 'drew': 167,\n",
       " 'come': 118,\n",
       " 'wolfga': 596,\n",
       " 'high': 249,\n",
       " 'hi': 248,\n",
       " 'toples': 543,\n",
       " 'beware': 68,\n",
       " 'august': 58,\n",
       " 'ev': 181,\n",
       " 'ame': 38,\n",
       " 'depend': 148,\n",
       " 'wizard': 594,\n",
       " 'inspir': 265,\n",
       " 'adam': 23,\n",
       " 'deserv': 149,\n",
       " 'old': 383,\n",
       " '44': 13,\n",
       " 'wild': 592,\n",
       " 'dear': 142,\n",
       " 'welcom': 586,\n",
       " 'underw': 558,\n",
       " 'nam': 363,\n",
       " 'robin': 448,\n",
       " 'possib': 410,\n",
       " 'holy': 253,\n",
       " 'surrou': 513,\n",
       " 'breakd': 82,\n",
       " 'ugh': 554,\n",
       " 'ripe': 445,\n",
       " 'yeah': 614,\n",
       " 'actual': 21,\n",
       " 'cop': 125,\n",
       " 'arnold': 50,\n",
       " 'stil': 503,\n",
       " 'gordie': 231,\n",
       " 'confuc': 121,\n",
       " 'despit': 151,\n",
       " 'paybac': 392,\n",
       " 'spawn': 490,\n",
       " 'mandin': 327,\n",
       " 'ab': 17,\n",
       " 'ladybu': 302,\n",
       " 'shagad': 474,\n",
       " 'chill': 105,\n",
       " 'retros': 440,\n",
       " 'years': 615,\n",
       " 'phaedr': 396,\n",
       " 'pe': 393,\n",
       " 'mont': 354,\n",
       " 'retell': 439,\n",
       " 'violen': 570,\n",
       " 'lauded': 304,\n",
       " 'dou': 165,\n",
       " 'warner': 579,\n",
       " 'discon': 160,\n",
       " 'runnin': 452,\n",
       " 'chi': 104,\n",
       " 'sandra': 456,\n",
       " 'mirama': 350,\n",
       " 'starin': 497,\n",
       " 'way': 583,\n",
       " 'arrivi': 51,\n",
       " 'att': 56,\n",
       " 'al': 29,\n",
       " 'bl': 74,\n",
       " 'michae': 344,\n",
       " 'midway': 346,\n",
       " '14': 2,\n",
       " 'yea': 613,\n",
       " 'plunke': 405,\n",
       " 'terren': 527,\n",
       " 'crie': 134,\n",
       " 'saw': 459,\n",
       " '18': 3,\n",
       " '1989': 8,\n",
       " 'woody': 601,\n",
       " 'fantas': 192,\n",
       " 'movie': 356,\n",
       " 'whethe': 590,\n",
       " 'viking': 569,\n",
       " 'appare': 45,\n",
       " 'ge': 219,\n",
       " 'cradle': 132,\n",
       " 'disill': 161,\n",
       " 'kate': 290,\n",
       " 'blatan': 75,\n",
       " 'won': 597,\n",
       " 'fo': 208,\n",
       " 'gia': 225,\n",
       " 'readin': 428,\n",
       " 'godzil': 229,\n",
       " 'ad': 22,\n",
       " 'suicid': 508,\n",
       " 'jo': 283,\n",
       " 'silly': 482,\n",
       " 'phi': 398,\n",
       " 'aff': 25,\n",
       " 'filmma': 203,\n",
       " 'wat': 582,\n",
       " '80': 15,\n",
       " 'joe': 284,\n",
       " 'gen': 220,\n",
       " 'flubbe': 207,\n",
       " 'gregg': 235,\n",
       " 'tina': 536,\n",
       " 'phil': 399,\n",
       " 'writer': 608,\n",
       " 'guilt': 239,\n",
       " '200': 12,\n",
       " 'ci': 109,\n",
       " 'new': 367,\n",
       " 'kids': 293,\n",
       " 'tri': 549,\n",
       " 'sc': 461,\n",
       " 'jack': 269,\n",
       " 'son': 488,\n",
       " 'wesley': 588,\n",
       " 'sensel': 468,\n",
       " 'came': 92,\n",
       " 'ready': 429,\n",
       " 'pokemo': 407,\n",
       " 'frie': 214,\n",
       " 'attent': 57,\n",
       " 'house': 255,\n",
       " 'fit': 206,\n",
       " 'nothin': 373,\n",
       " 'mugsho': 361,\n",
       " 'prepos': 414,\n",
       " 'starri': 498,\n",
       " 'bu': 87,\n",
       " 'chris': 108,\n",
       " 'soldie': 485,\n",
       " 'cashin': 97,\n",
       " 'girl': 226,\n",
       " 'cou': 128,\n",
       " 'long': 317,\n",
       " 'severa': 471,\n",
       " 'talk': 522,\n",
       " 'conven': 124,\n",
       " 'dol': 163,\n",
       " 'jet': 281,\n",
       " 'li': 309,\n",
       " 'ni': 368,\n",
       " 'weighe': 584,\n",
       " 'use': 561,\n",
       " 'hav': 244,\n",
       " 'pl': 401,\n",
       " 'everyt': 184,\n",
       " 'phew': 397,\n",
       " 'fre': 210,\n",
       " 'hap': 241,\n",
       " 'hear': 245,\n",
       " 'varsit': 565,\n",
       " 'missio': 351,\n",
       " 'fri': 213,\n",
       " 'dirty': 158,\n",
       " 'mr': 358,\n",
       " 'wonder': 599,\n",
       " 'war': 578,\n",
       " 'sill': 481,\n",
       " 'stephe': 500,\n",
       " 'die': 155,\n",
       " 'admi': 24,\n",
       " 'sylves': 518,\n",
       " 'bats': 64,\n",
       " 'gu': 237,\n",
       " 'meg': 337,\n",
       " 'ry': 453,\n",
       " 'disa': 159,\n",
       " 'isn': 268,\n",
       " 'idle': 259,\n",
       " 'suav': 507,\n",
       " 'ironic': 267,\n",
       " '1990s': 10,\n",
       " 'delica': 146,\n",
       " 'ho': 251,\n",
       " 'vegas': 567,\n",
       " 'burnt': 90,\n",
       " 'claire': 115,\n",
       " 'aspiri': 53,\n",
       " 'easely': 170,\n",
       " 'paul': 391,\n",
       " 'fact': 190,\n",
       " 'overbl': 388,\n",
       " 'return': 441,\n",
       " 'spoile': 492,\n",
       " 'fe': 194,\n",
       " 'went': 587,\n",
       " 'gi': 224,\n",
       " 'days': 141,\n",
       " 'lif': 311,\n",
       " 'renee': 436,\n",
       " 'writin': 609,\n",
       " 'teenag': 525,\n",
       " 'walken': 575,\n",
       " 'inn': 263,\n",
       " 'frank': 209,\n",
       " 'woof': 602,\n",
       " 'ah': 27,\n",
       " 'stallo': 495,\n",
       " 'numb': 376,\n",
       " 'everyb': 182,\n",
       " 'martia': 330,\n",
       " 'arye': 52,\n",
       " 'birthd': 73,\n",
       " 'uni': 560,\n",
       " 'post': 411,\n",
       " 'summer': 511,\n",
       " 'ju': 287,\n",
       " 'alcoho': 32,\n",
       " 'words': 603,\n",
       " 'virus': 571,\n",
       " 'barb': 61,\n",
       " 'humani': 257,\n",
       " 'absolu': 18,\n",
       " 'book': 78,\n",
       " 'cr': 131,\n",
       " 'bicent': 69,\n",
       " 'stars': 499,\n",
       " 'nostal': 371,\n",
       " 'tommy': 541,\n",
       " 'boy': 81,\n",
       " 'ki': 292,\n",
       " 'salari': 454}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the vectorizer on training data \n",
    "vec.fit(y_train)\n",
    "vec.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Countvectorizer()``` has converted the documents into a set of unique words alphabetically sorted and indexed.\n",
    "\n",
    "\n",
    "**Stop Words**\n",
    "\n",
    "We can see a few trivial words such as  'and','is','of', etc. These words don't really make any difference in classyfying a document. These are called **stop words**. So we would like to get rid of them. \n",
    "\n",
    "We can remove them by passing a parameter stop_words='english' while instantiating ```Countvectorizer()``` as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'comm': 119,\n",
       " 'whew': 591,\n",
       " 'steven': 502,\n",
       " 've': 566,\n",
       " 'hate': 243,\n",
       " 'susan': 514,\n",
       " 'ingred': 262,\n",
       " 'capsul': 93,\n",
       " 'bruce': 86,\n",
       " 'mus': 362,\n",
       " 'ca': 91,\n",
       " 'expand': 186,\n",
       " 'waitin': 574,\n",
       " 'becaus': 67,\n",
       " '197': 6,\n",
       " 'sha': 473,\n",
       " 'today': 539,\n",
       " 'note': 372,\n",
       " 'bug': 89,\n",
       " 'near': 366,\n",
       " 'jerry': 279,\n",
       " 'ha': 240,\n",
       " 'da': 136,\n",
       " 'seen': 466,\n",
       " 'althou': 37,\n",
       " 'fee': 195,\n",
       " 'ta': 520,\n",
       " 'perhap': 395,\n",
       " 'kadosh': 289,\n",
       " 'fi': 199,\n",
       " 'hum': 256,\n",
       " 'good': 230,\n",
       " 'curdle': 135,\n",
       " 'jarvis': 275,\n",
       " 'richar': 444,\n",
       " 'bree': 83,\n",
       " 'titani': 537,\n",
       " 'americ': 39,\n",
       " '19': 4,\n",
       " 'wa': 573,\n",
       " 'ma': 323,\n",
       " 'arg': 48,\n",
       " 'felix': 198,\n",
       " 'know': 296,\n",
       " 'living': 315,\n",
       " 'oh': 380,\n",
       " 'writte': 610,\n",
       " 'octobe': 379,\n",
       " 'ok': 381,\n",
       " 'traili': 546,\n",
       " 'll': 316,\n",
       " 'accord': 19,\n",
       " 'nottin': 375,\n",
       " 'slaver': 483,\n",
       " 'reflec': 433,\n",
       " 'mu': 359,\n",
       " 'gr': 234,\n",
       " 'lean': 306,\n",
       " '1992': 11,\n",
       " 'expect': 187,\n",
       " 'je': 277,\n",
       " 'aggres': 26,\n",
       " 'gothic': 233,\n",
       " 'ess': 180,\n",
       " 'clue': 117,\n",
       " 'dr': 166,\n",
       " 'people': 394,\n",
       " 'ce': 100,\n",
       " 'synops': 519,\n",
       " 'matthe': 334,\n",
       " 'ro': 446,\n",
       " 'confes': 120,\n",
       " 'john': 285,\n",
       " 'fu': 216,\n",
       " 'produc': 417,\n",
       " 'reme': 434,\n",
       " 'review': 442,\n",
       " 'fil': 201,\n",
       " 'ne': 365,\n",
       " 'contac': 122,\n",
       " 'like': 313,\n",
       " 'bo': 76,\n",
       " 'jonath': 286,\n",
       " 'scream': 464,\n",
       " 'leonar': 307,\n",
       " 'voices': 572,\n",
       " 'alien': 34,\n",
       " 'ultra': 555,\n",
       " 'elmore': 174,\n",
       " 'bob': 77,\n",
       " 'th': 528,\n",
       " 'martin': 331,\n",
       " 'carolc': 95,\n",
       " 'par': 390,\n",
       " 'contra': 123,\n",
       " 'elizab': 173,\n",
       " 'making': 325,\n",
       " 'set': 469,\n",
       " 'ex': 185,\n",
       " 'en': 175,\n",
       " 'someti': 487,\n",
       " 'didn': 154,\n",
       " '118': 0,\n",
       " 'mi': 343,\n",
       " 'okay': 382,\n",
       " 'taking': 521,\n",
       " 'garry': 217,\n",
       " 'reca': 431,\n",
       " 'oliver': 384,\n",
       " 'tim': 534,\n",
       " 'bowfin': 80,\n",
       " 'pulp': 419,\n",
       " 'gattac': 218,\n",
       " 'availa': 59,\n",
       " 'melvin': 338,\n",
       " 'plot': 404,\n",
       " 'defend': 145,\n",
       " 'jake': 272,\n",
       " '199': 9,\n",
       " 'shakes': 475,\n",
       " 'ye': 612,\n",
       " 'bri': 84,\n",
       " 'som': 486,\n",
       " 'fel': 197,\n",
       " 'george': 221,\n",
       " 'countr': 129,\n",
       " 'freque': 212,\n",
       " 'psycho': 418,\n",
       " 'jackie': 270,\n",
       " 'direct': 157,\n",
       " 'disney': 162,\n",
       " 'ri': 443,\n",
       " 'ladies': 301,\n",
       " 'billed': 71,\n",
       " 'apr': 46,\n",
       " 'based': 63,\n",
       " 'quenti': 421,\n",
       " 'david': 140,\n",
       " 'seven': 470,\n",
       " 'ast': 55,\n",
       " 'lake': 303,\n",
       " 'lot': 320,\n",
       " 'desper': 150,\n",
       " 'wond': 598,\n",
       " 'scarfa': 462,\n",
       " 'trekki': 548,\n",
       " 'polloc': 408,\n",
       " 'warnin': 580,\n",
       " 'sho': 476,\n",
       " 'deep': 144,\n",
       " 'vampir': 563,\n",
       " 'swashb': 516,\n",
       " 'rebel': 430,\n",
       " 'cather': 99,\n",
       " 'love': 321,\n",
       " 'cost': 127,\n",
       " 'jay': 276,\n",
       " 'sci': 463,\n",
       " 'let': 308,\n",
       " 'rated': 425,\n",
       " 'thi': 529,\n",
       " 'saving': 458,\n",
       " '50': 14,\n",
       " 'tr': 545,\n",
       " 'apocal': 43,\n",
       " 'earlie': 169,\n",
       " 'just': 288,\n",
       " 'barely': 62,\n",
       " 'want': 577,\n",
       " 'losing': 319,\n",
       " 'muc': 360,\n",
       " 'po': 406,\n",
       " 'gues': 238,\n",
       " 'maybe': 335,\n",
       " 'krippe': 298,\n",
       " 'si': 479,\n",
       " 'luckil': 322,\n",
       " 'cir': 112,\n",
       " 'nosfer': 370,\n",
       " 'sick': 480,\n",
       " 'studio': 505,\n",
       " 'james': 274,\n",
       " 'ene': 176,\n",
       " 'tbwp': 524,\n",
       " 'don': 164,\n",
       " 'gettin': 223,\n",
       " 'mat': 333,\n",
       " 'danger': 137,\n",
       " 'imagin': 260,\n",
       " 'steve': 501,\n",
       " 'fir': 205,\n",
       " 'harmle': 242,\n",
       " 'god': 228,\n",
       " 'se': 465,\n",
       " 'aliens': 35,\n",
       " 'metro': 342,\n",
       " 'eric': 178,\n",
       " 'hedwig': 246,\n",
       " 'underr': 557,\n",
       " 'fai': 191,\n",
       " 'sta': 494,\n",
       " 'eyes': 188,\n",
       " 'jamaic': 273,\n",
       " 'denzel': 147,\n",
       " 'whenev': 589,\n",
       " 'rememb': 435,\n",
       " 'errol': 179,\n",
       " 'summar': 510,\n",
       " 'hollyw': 252,\n",
       " 'say': 460,\n",
       " 'kevin': 291,\n",
       " 'rent': 438,\n",
       " 'everyo': 183,\n",
       " 'oct': 378,\n",
       " 'brian': 85,\n",
       " 'cri': 133,\n",
       " 'magnol': 324,\n",
       " 'grou': 236,\n",
       " 'mickey': 345,\n",
       " 'mira': 349,\n",
       " 'welc': 585,\n",
       " 'origin': 387,\n",
       " 'did': 153,\n",
       " 'yo': 616,\n",
       " 'feel': 196,\n",
       " 'ra': 423,\n",
       " 'tarzan': 523,\n",
       " 'castin': 98,\n",
       " 'histor': 250,\n",
       " 'natura': 364,\n",
       " 'jean': 278,\n",
       " 'pr': 412,\n",
       " 'mig': 347,\n",
       " 'kirk': 294,\n",
       " 'man': 326,\n",
       " 'su': 506,\n",
       " 'jacque': 271,\n",
       " 'findin': 204,\n",
       " 'supp': 512,\n",
       " 'tone': 542,\n",
       " 'op': 386,\n",
       " 'dec': 143,\n",
       " 'la': 299,\n",
       " 'renown': 437,\n",
       " '198': 7,\n",
       " 'look': 318,\n",
       " 'pleasa': 403,\n",
       " 'big': 70,\n",
       " 'time': 535,\n",
       " 'charli': 103,\n",
       " 'field': 200,\n",
       " 'alchem': 31,\n",
       " 'wo': 595,\n",
       " 'anothe': 42,\n",
       " 'freq': 211,\n",
       " 'ch': 102,\n",
       " 'mary': 332,\n",
       " 'touchs': 544,\n",
       " 'run': 451,\n",
       " 'pre': 413,\n",
       " 'star': 496,\n",
       " 'usuall': 562,\n",
       " 'priv': 415,\n",
       " 'sam': 455,\n",
       " 'copyri': 126,\n",
       " 'playwr': 402,\n",
       " 'titant': 538,\n",
       " 'sw': 515,\n",
       " 'modern': 353,\n",
       " 'vannes': 564,\n",
       " 'kn': 295,\n",
       " 'coup': 130,\n",
       " 'pa': 389,\n",
       " 'dimens': 156,\n",
       " 'edward': 172,\n",
       " 'movi': 355,\n",
       " 'life': 312,\n",
       " 'kolya': 297,\n",
       " 'anna': 41,\n",
       " 'devote': 152,\n",
       " 'film': 202,\n",
       " 'quiz': 422,\n",
       " 'jim': 282,\n",
       " 'men': 339,\n",
       " 'ralph': 424,\n",
       " 'true': 550,\n",
       " 'dark': 138,\n",
       " 'recent': 432,\n",
       " 'roboco': 449,\n",
       " 'probab': 416,\n",
       " 'cinema': 111,\n",
       " 'chr': 107,\n",
       " 'le': 305,\n",
       " 'tempe': 526,\n",
       " 'fa': 189,\n",
       " 'fritz': 215,\n",
       " 'vetera': 568,\n",
       " 'satiri': 457,\n",
       " 'cl': 114,\n",
       " 'drivin': 168,\n",
       " 'mars': 329,\n",
       " 'showgi': 478,\n",
       " 'unh': 559,\n",
       " 'stran': 504,\n",
       " 'albert': 30,\n",
       " 'mo': 352,\n",
       " 'razor': 426,\n",
       " 'pitch': 400,\n",
       " 'cho': 106,\n",
       " 'assume': 54,\n",
       " 'qu': 420,\n",
       " 'bea': 66,\n",
       " 'sm': 484,\n",
       " 'warren': 581,\n",
       " 'clint': 116,\n",
       " 'meet': 336,\n",
       " 'buffal': 88,\n",
       " 'sum': 509,\n",
       " 'mike': 348,\n",
       " 'ind': 261,\n",
       " 'throug': 531,\n",
       " 'wong': 600,\n",
       " 'city': 113,\n",
       " 'robert': 447,\n",
       " 'rog': 450,\n",
       " 'labell': 300,\n",
       " 'airpla': 28,\n",
       " 'apollo': 44,\n",
       " 'carla': 94,\n",
       " 'cine': 110,\n",
       " '191': 5,\n",
       " 'st': 493,\n",
       " 'niagar': 369,\n",
       " 'notice': 374,\n",
       " 'actu': 20,\n",
       " 'gere': 222,\n",
       " 'wyatt': 611,\n",
       " 'zoo': 617,\n",
       " 'billy': 72,\n",
       " 'tibet': 533,\n",
       " 'swea': 517,\n",
       " 'armage': 49,\n",
       " 'id': 258,\n",
       " 'meteor': 341,\n",
       " 'books': 79,\n",
       " 'eddie': 171,\n",
       " 'uncomp': 556,\n",
       " 'ent': 177,\n",
       " 'gordon': 232,\n",
       " 'anasta': 40,\n",
       " 'marie': 328,\n",
       " 'insane': 264,\n",
       " 'allen': 36,\n",
       " 'trees': 547,\n",
       " 'wish': 593,\n",
       " 'lisa': 314,\n",
       " 'thri': 530,\n",
       " 'wow': 607,\n",
       " 'glory': 227,\n",
       " 'truman': 551,\n",
       " 'jessic': 280,\n",
       " 'certai': 101,\n",
       " 'woul': 606,\n",
       " 'libby': 310,\n",
       " 'far': 193,\n",
       " 'try': 552,\n",
       " 'workin': 604,\n",
       " 'spice': 491,\n",
       " 'read': 427,\n",
       " 'shou': 477,\n",
       " 'bad': 60,\n",
       " 'darren': 139,\n",
       " 'int': 266,\n",
       " 'sens': 467,\n",
       " 'battle': 65,\n",
       " 'hey': 247,\n",
       " 'movies': 357,\n",
       " 'sh': 472,\n",
       " 'ar': 47,\n",
       " 'mercur': 340,\n",
       " 'hots': 254,\n",
       " 'sp': 489,\n",
       " 'ti': 532,\n",
       " 'ob': 377,\n",
       " 'porter': 409,\n",
       " 'wou': 605,\n",
       " 'onl': 385,\n",
       " 'tv': 553,\n",
       " '13': 1,\n",
       " 'alexan': 33,\n",
       " '8mm': 16,\n",
       " 'carry': 96,\n",
       " 'tomb': 540,\n",
       " 'walt': 576,\n",
       " 'drew': 167,\n",
       " 'come': 118,\n",
       " 'wolfga': 596,\n",
       " 'high': 249,\n",
       " 'hi': 248,\n",
       " 'toples': 543,\n",
       " 'beware': 68,\n",
       " 'august': 58,\n",
       " 'ev': 181,\n",
       " 'ame': 38,\n",
       " 'depend': 148,\n",
       " 'wizard': 594,\n",
       " 'inspir': 265,\n",
       " 'adam': 23,\n",
       " 'deserv': 149,\n",
       " 'old': 383,\n",
       " '44': 13,\n",
       " 'wild': 592,\n",
       " 'dear': 142,\n",
       " 'welcom': 586,\n",
       " 'underw': 558,\n",
       " 'nam': 363,\n",
       " 'robin': 448,\n",
       " 'possib': 410,\n",
       " 'holy': 253,\n",
       " 'surrou': 513,\n",
       " 'breakd': 82,\n",
       " 'ugh': 554,\n",
       " 'ripe': 445,\n",
       " 'yeah': 614,\n",
       " 'actual': 21,\n",
       " 'cop': 125,\n",
       " 'arnold': 50,\n",
       " 'stil': 503,\n",
       " 'gordie': 231,\n",
       " 'confuc': 121,\n",
       " 'despit': 151,\n",
       " 'paybac': 392,\n",
       " 'spawn': 490,\n",
       " 'mandin': 327,\n",
       " 'ab': 17,\n",
       " 'ladybu': 302,\n",
       " 'shagad': 474,\n",
       " 'chill': 105,\n",
       " 'retros': 440,\n",
       " 'years': 615,\n",
       " 'phaedr': 396,\n",
       " 'pe': 393,\n",
       " 'mont': 354,\n",
       " 'retell': 439,\n",
       " 'violen': 570,\n",
       " 'lauded': 304,\n",
       " 'dou': 165,\n",
       " 'warner': 579,\n",
       " 'discon': 160,\n",
       " 'runnin': 452,\n",
       " 'chi': 104,\n",
       " 'sandra': 456,\n",
       " 'mirama': 350,\n",
       " 'starin': 497,\n",
       " 'way': 583,\n",
       " 'arrivi': 51,\n",
       " 'att': 56,\n",
       " 'al': 29,\n",
       " 'bl': 74,\n",
       " 'michae': 344,\n",
       " 'midway': 346,\n",
       " '14': 2,\n",
       " 'yea': 613,\n",
       " 'plunke': 405,\n",
       " 'terren': 527,\n",
       " 'crie': 134,\n",
       " 'saw': 459,\n",
       " '18': 3,\n",
       " '1989': 8,\n",
       " 'woody': 601,\n",
       " 'fantas': 192,\n",
       " 'movie': 356,\n",
       " 'whethe': 590,\n",
       " 'viking': 569,\n",
       " 'appare': 45,\n",
       " 'ge': 219,\n",
       " 'cradle': 132,\n",
       " 'disill': 161,\n",
       " 'kate': 290,\n",
       " 'blatan': 75,\n",
       " 'won': 597,\n",
       " 'fo': 208,\n",
       " 'gia': 225,\n",
       " 'readin': 428,\n",
       " 'godzil': 229,\n",
       " 'ad': 22,\n",
       " 'suicid': 508,\n",
       " 'jo': 283,\n",
       " 'silly': 482,\n",
       " 'phi': 398,\n",
       " 'aff': 25,\n",
       " 'filmma': 203,\n",
       " 'wat': 582,\n",
       " '80': 15,\n",
       " 'joe': 284,\n",
       " 'gen': 220,\n",
       " 'flubbe': 207,\n",
       " 'gregg': 235,\n",
       " 'tina': 536,\n",
       " 'phil': 399,\n",
       " 'writer': 608,\n",
       " 'guilt': 239,\n",
       " '200': 12,\n",
       " 'ci': 109,\n",
       " 'new': 367,\n",
       " 'kids': 293,\n",
       " 'tri': 549,\n",
       " 'sc': 461,\n",
       " 'jack': 269,\n",
       " 'son': 488,\n",
       " 'wesley': 588,\n",
       " 'sensel': 468,\n",
       " 'came': 92,\n",
       " 'ready': 429,\n",
       " 'pokemo': 407,\n",
       " 'frie': 214,\n",
       " 'attent': 57,\n",
       " 'house': 255,\n",
       " 'fit': 206,\n",
       " 'nothin': 373,\n",
       " 'mugsho': 361,\n",
       " 'prepos': 414,\n",
       " 'starri': 498,\n",
       " 'bu': 87,\n",
       " 'chris': 108,\n",
       " 'soldie': 485,\n",
       " 'cashin': 97,\n",
       " 'girl': 226,\n",
       " 'cou': 128,\n",
       " 'long': 317,\n",
       " 'severa': 471,\n",
       " 'talk': 522,\n",
       " 'conven': 124,\n",
       " 'dol': 163,\n",
       " 'jet': 281,\n",
       " 'li': 309,\n",
       " 'ni': 368,\n",
       " 'weighe': 584,\n",
       " 'use': 561,\n",
       " 'hav': 244,\n",
       " 'pl': 401,\n",
       " 'everyt': 184,\n",
       " 'phew': 397,\n",
       " 'fre': 210,\n",
       " 'hap': 241,\n",
       " 'hear': 245,\n",
       " 'varsit': 565,\n",
       " 'missio': 351,\n",
       " 'fri': 213,\n",
       " 'dirty': 158,\n",
       " 'mr': 358,\n",
       " 'wonder': 599,\n",
       " 'war': 578,\n",
       " 'sill': 481,\n",
       " 'stephe': 500,\n",
       " 'die': 155,\n",
       " 'admi': 24,\n",
       " 'sylves': 518,\n",
       " 'bats': 64,\n",
       " 'gu': 237,\n",
       " 'meg': 337,\n",
       " 'ry': 453,\n",
       " 'disa': 159,\n",
       " 'isn': 268,\n",
       " 'idle': 259,\n",
       " 'suav': 507,\n",
       " 'ironic': 267,\n",
       " '1990s': 10,\n",
       " 'delica': 146,\n",
       " 'ho': 251,\n",
       " 'vegas': 567,\n",
       " 'burnt': 90,\n",
       " 'claire': 115,\n",
       " 'aspiri': 53,\n",
       " 'easely': 170,\n",
       " 'paul': 391,\n",
       " 'fact': 190,\n",
       " 'overbl': 388,\n",
       " 'return': 441,\n",
       " 'spoile': 492,\n",
       " 'fe': 194,\n",
       " 'went': 587,\n",
       " 'gi': 224,\n",
       " 'days': 141,\n",
       " 'lif': 311,\n",
       " 'renee': 436,\n",
       " 'writin': 609,\n",
       " 'teenag': 525,\n",
       " 'walken': 575,\n",
       " 'inn': 263,\n",
       " 'frank': 209,\n",
       " 'woof': 602,\n",
       " 'ah': 27,\n",
       " 'stallo': 495,\n",
       " 'numb': 376,\n",
       " 'everyb': 182,\n",
       " 'martia': 330,\n",
       " 'arye': 52,\n",
       " 'birthd': 73,\n",
       " 'uni': 560,\n",
       " 'post': 411,\n",
       " 'summer': 511,\n",
       " 'ju': 287,\n",
       " 'alcoho': 32,\n",
       " 'words': 603,\n",
       " 'virus': 571,\n",
       " 'barb': 61,\n",
       " 'humani': 257,\n",
       " 'absolu': 18,\n",
       " 'book': 78,\n",
       " 'cr': 131,\n",
       " 'bicent': 69,\n",
       " 'stars': 499,\n",
       " 'nostal': 371,\n",
       " 'tommy': 541,\n",
       " 'boy': 81,\n",
       " 'ki': 292,\n",
       " 'salari': 454}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting the vectorizer on training data again\n",
    "# removing the stop words this time\n",
    "vec = CountVectorizer(stop_words='english')\n",
    "vec.fit(y_train)\n",
    "vec.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the vocabulary has reduced to 12 from 15. Another way of printing the 'vocabulary' is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['118', '13', '14', '18', '19', '191', '197', '198', '1989', '199', '1990s', '1992', '200', '44', '50', '80', '8mm', 'ab', 'absolu', 'accord', 'actu', 'actual', 'ad', 'adam', 'admi', 'aff', 'aggres', 'ah', 'airpla', 'al', 'albert', 'alchem', 'alcoho', 'alexan', 'alien', 'aliens', 'allen', 'althou', 'ame', 'americ', 'anasta', 'anna', 'anothe', 'apocal', 'apollo', 'appare', 'apr', 'ar', 'arg', 'armage', 'arnold', 'arrivi', 'arye', 'aspiri', 'assume', 'ast', 'att', 'attent', 'august', 'availa', 'bad', 'barb', 'barely', 'based', 'bats', 'battle', 'bea', 'becaus', 'beware', 'bicent', 'big', 'billed', 'billy', 'birthd', 'bl', 'blatan', 'bo', 'bob', 'book', 'books', 'bowfin', 'boy', 'breakd', 'bree', 'bri', 'brian', 'bruce', 'bu', 'buffal', 'bug', 'burnt', 'ca', 'came', 'capsul', 'carla', 'carolc', 'carry', 'cashin', 'castin', 'cather', 'ce', 'certai', 'ch', 'charli', 'chi', 'chill', 'cho', 'chr', 'chris', 'ci', 'cine', 'cinema', 'cir', 'city', 'cl', 'claire', 'clint', 'clue', 'come', 'comm', 'confes', 'confuc', 'contac', 'contra', 'conven', 'cop', 'copyri', 'cost', 'cou', 'countr', 'coup', 'cr', 'cradle', 'cri', 'crie', 'curdle', 'da', 'danger', 'dark', 'darren', 'david', 'days', 'dear', 'dec', 'deep', 'defend', 'delica', 'denzel', 'depend', 'deserv', 'desper', 'despit', 'devote', 'did', 'didn', 'die', 'dimens', 'direct', 'dirty', 'disa', 'discon', 'disill', 'disney', 'dol', 'don', 'dou', 'dr', 'drew', 'drivin', 'earlie', 'easely', 'eddie', 'edward', 'elizab', 'elmore', 'en', 'ene', 'ent', 'eric', 'errol', 'ess', 'ev', 'everyb', 'everyo', 'everyt', 'ex', 'expand', 'expect', 'eyes', 'fa', 'fact', 'fai', 'fantas', 'far', 'fe', 'fee', 'feel', 'fel', 'felix', 'fi', 'field', 'fil', 'film', 'filmma', 'findin', 'fir', 'fit', 'flubbe', 'fo', 'frank', 'fre', 'freq', 'freque', 'fri', 'frie', 'fritz', 'fu', 'garry', 'gattac', 'ge', 'gen', 'george', 'gere', 'gettin', 'gi', 'gia', 'girl', 'glory', 'god', 'godzil', 'good', 'gordie', 'gordon', 'gothic', 'gr', 'gregg', 'grou', 'gu', 'gues', 'guilt', 'ha', 'hap', 'harmle', 'hate', 'hav', 'hear', 'hedwig', 'hey', 'hi', 'high', 'histor', 'ho', 'hollyw', 'holy', 'hots', 'house', 'hum', 'humani', 'id', 'idle', 'imagin', 'ind', 'ingred', 'inn', 'insane', 'inspir', 'int', 'ironic', 'isn', 'jack', 'jackie', 'jacque', 'jake', 'jamaic', 'james', 'jarvis', 'jay', 'je', 'jean', 'jerry', 'jessic', 'jet', 'jim', 'jo', 'joe', 'john', 'jonath', 'ju', 'just', 'kadosh', 'kate', 'kevin', 'ki', 'kids', 'kirk', 'kn', 'know', 'kolya', 'krippe', 'la', 'labell', 'ladies', 'ladybu', 'lake', 'lauded', 'le', 'lean', 'leonar', 'let', 'li', 'libby', 'lif', 'life', 'like', 'lisa', 'living', 'll', 'long', 'look', 'losing', 'lot', 'love', 'luckil', 'ma', 'magnol', 'making', 'man', 'mandin', 'marie', 'mars', 'martia', 'martin', 'mary', 'mat', 'matthe', 'maybe', 'meet', 'meg', 'melvin', 'men', 'mercur', 'meteor', 'metro', 'mi', 'michae', 'mickey', 'midway', 'mig', 'mike', 'mira', 'mirama', 'missio', 'mo', 'modern', 'mont', 'movi', 'movie', 'movies', 'mr', 'mu', 'muc', 'mugsho', 'mus', 'nam', 'natura', 'ne', 'near', 'new', 'ni', 'niagar', 'nosfer', 'nostal', 'note', 'nothin', 'notice', 'nottin', 'numb', 'ob', 'oct', 'octobe', 'oh', 'ok', 'okay', 'old', 'oliver', 'onl', 'op', 'origin', 'overbl', 'pa', 'par', 'paul', 'paybac', 'pe', 'people', 'perhap', 'phaedr', 'phew', 'phi', 'phil', 'pitch', 'pl', 'playwr', 'pleasa', 'plot', 'plunke', 'po', 'pokemo', 'polloc', 'porter', 'possib', 'post', 'pr', 'pre', 'prepos', 'priv', 'probab', 'produc', 'psycho', 'pulp', 'qu', 'quenti', 'quiz', 'ra', 'ralph', 'rated', 'razor', 'read', 'readin', 'ready', 'rebel', 'reca', 'recent', 'reflec', 'reme', 'rememb', 'renee', 'renown', 'rent', 'retell', 'retros', 'return', 'review', 'ri', 'richar', 'ripe', 'ro', 'robert', 'robin', 'roboco', 'rog', 'run', 'runnin', 'ry', 'salari', 'sam', 'sandra', 'satiri', 'saving', 'saw', 'say', 'sc', 'scarfa', 'sci', 'scream', 'se', 'seen', 'sens', 'sensel', 'set', 'seven', 'severa', 'sh', 'sha', 'shagad', 'shakes', 'sho', 'shou', 'showgi', 'si', 'sick', 'sill', 'silly', 'slaver', 'sm', 'soldie', 'som', 'someti', 'son', 'sp', 'spawn', 'spice', 'spoile', 'st', 'sta', 'stallo', 'star', 'starin', 'starri', 'stars', 'stephe', 'steve', 'steven', 'stil', 'stran', 'studio', 'su', 'suav', 'suicid', 'sum', 'summar', 'summer', 'supp', 'surrou', 'susan', 'sw', 'swashb', 'swea', 'sylves', 'synops', 'ta', 'taking', 'talk', 'tarzan', 'tbwp', 'teenag', 'tempe', 'terren', 'th', 'thi', 'thri', 'throug', 'ti', 'tibet', 'tim', 'time', 'tina', 'titani', 'titant', 'today', 'tomb', 'tommy', 'tone', 'toples', 'touchs', 'tr', 'traili', 'trees', 'trekki', 'tri', 'true', 'truman', 'try', 'tv', 'ugh', 'ultra', 'uncomp', 'underr', 'underw', 'unh', 'uni', 'use', 'usuall', 'vampir', 'vannes', 'varsit', 've', 'vegas', 'vetera', 'viking', 'violen', 'virus', 'voices', 'wa', 'waitin', 'walken', 'walt', 'want', 'war', 'warner', 'warnin', 'warren', 'wat', 'way', 'weighe', 'welc', 'welcom', 'went', 'wesley', 'whenev', 'whethe', 'whew', 'wild', 'wish', 'wizard', 'wo', 'wolfga', 'won', 'wond', 'wonder', 'wong', 'woody', 'woof', 'words', 'workin', 'wou', 'woul', 'wow', 'writer', 'writin', 'writte', 'wyatt', 'ye', 'yea', 'yeah', 'years', 'yo', 'zoo']\n",
      "618\n"
     ]
    }
   ],
   "source": [
    "# printing feature names\n",
    "print(vec.get_feature_names())\n",
    "print(len(vec.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our final dictionary is made of 12 words (after discarding the stop words). Now, to do classification, we need to represent all the documents with these words (or tokens) as features. \n",
    "\n",
    "Every document will be converted into a *feature vector* representing presence of these words in that document. Let's convert each of our training documents in to a feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-6853704b7178>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# another way of representing the features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX_transformed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mX_transformed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1248\u001b[0m         \u001b[1;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1249\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1250\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1251\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1109\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \"\"\"\n\u001b[0;32m     68\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maccent_function\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccent_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "# another way of representing the features\n",
    "X_transformed = vec.transform(X_train)\n",
    "X_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see X_tranformed is a 5 x 12 **sparse matrix**. It has 5 rows for each of our 5 documents and 12 columns each \n",
    "for one word of the dictionary which we just created. Let us print X_transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_transformed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-cf348ef1b299>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_transformed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X_transformed' is not defined"
     ]
    }
   ],
   "source": [
    "print(X_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This representation can be understood as follows:\n",
    "\n",
    "Consider first 4 rows of the output: (0,2), (0,5), (0,7) and (0,11). It says that the first document (index 0) has \n",
    "7th , 2nd , 5th and 11th 'word' present in the document, and that they appear only\n",
    "once in the document- indicated by the right hand column entry. \n",
    "\n",
    "Similarly, consider the entry (4,4) (third from bottom). It says that the fifth document has the fifth word present twice. Indeed, the 5th word('good') appears twice in the 5th document. \n",
    "\n",
    "In real problems, you often work with large documents and vocabularies, and each document contains only a few words in the vocabulary. So it would be a waste of space to store the vocabulary in a typical dataframe, since most entries would be zero. Also, matrix products, additions etc. are much faster with sparse matrices. That's why we use sparse matrices to store the data.\n",
    "\n",
    "\n",
    "Let us convert this sparse matrix into a more easily interpretable array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_transformed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-a7887db853df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# converting transformed matrix back to an array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# note the high number of zeros\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mX_transformed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X_transformed' is not defined"
     ]
    }
   ],
   "source": [
    "# converting transformed matrix back to an array\n",
    "# note the high number of zeros\n",
    "X_transformed.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the dataset more readable, let us examine the vocabulary and the document-term matrix together in a pandas dataframe. The way to convert a matrix into a dataframe is ```pd.DataFrame(matrix, columns=columns)```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_transformed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-da543959119e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# converting matrix to dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m pd.DataFrame(X_transformed.toarray(), \n\u001b[0m\u001b[0;32m      3\u001b[0m              columns=vec.get_feature_names())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_transformed' is not defined"
     ]
    }
   ],
   "source": [
    "# converting matrix to dataframe\n",
    "pd.DataFrame(X_transformed.toarray(), \n",
    "             columns=vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table shows how many times a particular word occurs in document. In other words, this is a frequency table of the words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A corpus of documents can thus be represented by a matrix with one row per document and one column per\n",
    "token (e.g. word) occurring in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now import and transform the test data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>very good educational institution</td>\n",
       "      <td>education</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Document      Class\n",
       "0  very good educational institution  education"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test data\n",
    "test_docs = pd.read_csv('example_test.csv') \n",
    "test_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>very good educational institution</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Document  Class\n",
       "0  very good educational institution      1"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert label to a numerical variable\n",
    "test_docs['Class'] = test_docs.Class.map({'cinema':0, 'education':1})\n",
    "test_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test\n",
      "['very good educational institution']\n",
      "y_test\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "# convert to numpy array\n",
    "test_numpy_array = test_docs.values\n",
    "\n",
    "# split into X and y\n",
    "X_test = test_numpy_array[:,0]\n",
    "y_test = test_numpy_array[:,1]\n",
    "\n",
    "print(\"X_test\")\n",
    "print(X_test)\n",
    "print(\"y_test\")\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x12 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform the test data\n",
    "# note that you *never* fit on test data, only on training data\n",
    "# and only transform the test data\n",
    "X_test_transformed = vec.transform(X_test)\n",
    "X_test_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to non-sparse array\n",
    "X_test=X_test_transformed.toarray()\n",
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us summarise all we have done till now:\n",
    "\n",
    "- ```vect.fit(train)``` learns the vocabulary of the training data\n",
    "- ```vect.transform(train)``` uses the fitted vocabulary to build a document-term matrix from the training data\n",
    "- ```vect.transform(test)``` uses the fitted vocabulary to build a document-term matrix from the testing data (and ignores tokens it hasn't seen before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Building the Model: Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.32808399, 0.67191601]])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# building a multinomial NB model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# instantiate NB class\n",
    "mnb=MultinomialNB()\n",
    "\n",
    "# fitting the model on training data\n",
    "mnb.fit(X_transformed, y_train)\n",
    "\n",
    "# note that we are using the sparse matrix X_transformed, \n",
    "# though you can also use the non-sparse version\n",
    "# mnb.fit(X_transformed.toarray(), y_train) \n",
    "\n",
    "# predicting probabilities of test data\n",
    "proba = mnb.predict_proba(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability of test document belonging to class CINEMA [0.32808399]\n",
      "probability of test document belonging to class EDUCATION [0.67191601]\n"
     ]
    }
   ],
   "source": [
    "# probability of each class (test data)\n",
    "print(\"probability of test document belonging to class CINEMA\" , proba[:,0])\n",
    "print(\"probability of test document belonging to class EDUCATION\" , proba[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Building the Model: Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2326374, 0.7673626]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# instantiating bernoulli NB class\n",
    "bnb=BernoulliNB()\n",
    "\n",
    "# fitting the model\n",
    "bnb.fit(X_transformed, y_train)\n",
    "\n",
    "# also works\n",
    "# bnb.fit(X_transformed.toarray(), y_train)\n",
    "\n",
    "# predicting probability of test data\n",
    "bnb.predict_proba(X_test)\n",
    "prob_bnb = bnb.predict_proba(X_test)\n",
    "prob_bnb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next notebook, we will use Multinomial and Bernoulli Naive Bayes to solve an interesting real problem - classifying SMSes as spam or ham. We'll also see how to decide the optimal cutoff probability and evaluate the model.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
